Ruolo
Agisci come Codex (senior engineer/maintainer). Devi rendere il triage/decisione di convertibilità robusto e veloce su PDF molto diversi, riusando la cache+early-stop già implementate in core/page_cache.py e senza creare logiche duplicate tra moduli.

Vincolo architetturale (OBBLIGATORIO)
- Tutta la logica di “quali pagine scansionare e con che budget” deve vivere in core/page_cache.py (o in un helper chiamato da page_cache).
- core/triage.py e core/auto_convert.py devono CONSUMARE le CachedPage (con signal_score, price_like_count, cooccurrence_count, table_likelihood, ecc.) e non ricampionare in modo indipendente.

Obiettivo generale
1) Sampling adattivo per numero pagine (percentuale + cap) + distribuzione (step+jitter).
2) Ranking convertibilità con segnali cheap → poi words solo sulle candidate.
3) Retry deterministico quando i segnali sono insufficienti (too_few_rows/table_precheck_failed/low_score).
4) Output sempre (almeno XLSX diagnostico/summary), mai “nessun file”.
5) Mantieni performance: niente scan completo di default.
6) Mantieni compatibilità con GUI: progress callback stage coerenti (scan/convert/convert_page).

STEP 1 (OBBLIGATORIO, prima di tutto)
A) Sampling adattivo (percentuale + min/max + cap)
Implementa in page_cache:
- buckets generali (esempio):
  <=30: 30% (min8 max15)
  31–100: 15% (min12 max25)
  101–300: 10% (min20 max40)
  301–800: 3–5% (min30 max80)
  >800: 1% (min40 max120)
- selezione distribuita: step = num_pages/sample_count + jitter deterministico
- integra early-stop già esistente: early-stop SOLO se trovi >=K pagine forti e score alto.

B) Ranking cheap-first, expensive-later
- per tutte le pagine campionate: calcola score cheap da text/lines (numeric_density, currency_tokens, price_like_count, cooccurrence_count, mixed_code_count, penalty legal).
- solo per top_m candidate: extract_words() e aggiorna table_likelihood.
- restituisci top_k pages finali (8–20) a triage/auto_convert.

C) Retry deterministico
Se triage/fast-eval porta a too_few_rows / low_score / precheck_failed:
- Retry #1: aumenta sample_count (x2 o x3, cap 200) e rifai A–B.
- Se ancora fallisce: salva XLSX diagnostico (summary) e termina “gracefully”.

D) Output diagnostico sempre
Se conversione produce 0 righe totali o non seleziona parser:
- salva XLSX con SUMMARY che include:
  num_pages, sample_count, pages_sampled, top_k pages + score, parser_attempts + reasons.

Logging obbligatorio
- SAMPLE_COUNT num_pages=... sample_count=...
- TOP_K pages=[...] scores=[...]
- RETRY_SCAN reason=... old=... new=...
- DIAGNOSTIC_OUTPUT written=...

STEP 2 (DOPO che Step 1 passa smoke)
E) Targeted sweep + OCR budgeted
- Se Retry #1 fallisce: sweep 1 pagina ogni step largo (bucketed) finché trovi hit>threshold, poi espandi ±N e riprova.
- OCR solo su pagine ocr_candidate (max_ocr_pages = min(30, sample_count//2)), e solo quando cheap signals suggeriscono tabella raster.

Deliverable
- Patch su core/page_cache.py + core/triage.py + core/auto_convert.py + config.py
- Aggiorna test_smoke.py o aggiungi test minimale che verifica:
  * sample_count cambia con num_pages (40/150/350/900)
  * in too_few_rows scatta retry (log presente)
  * viene sempre generato un XLSX diagnostico se fallisce
